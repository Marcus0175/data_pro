{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HkHsfSz95V89",
        "outputId": "d97de47c-5feb-4ac8-90c3-c6e31f1761f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cấu trúc của dataset: 2365 hàng x 4 cột\n",
            "Các trường của dataset: ['index', 'user', 'item', 'rating']\n",
            "Tổng số lượng người dùng: 74\n",
            "Số người dùng đánh giá không khách quan: 8\n",
            "Tỉ lệ phần trăm: 10.81%\n",
            "\n",
            " Các người dùng đánh giá không khách quan:\n",
            "Người dùng 65: 1 lượt đánh giá, giá trị 1.0\n",
            "Người dùng 44: 1 lượt đánh giá, giá trị 4.5\n",
            "Người dùng 16: 2 lượt đánh giá, giá trị 5.0\n",
            "Người dùng 17: 1 lượt đánh giá, giá trị 3.0\n",
            "Người dùng 63: 1 lượt đánh giá, giá trị 3.0\n",
            "Người dùng 60: 1 lượt đánh giá, giá trị 4.0\n",
            "Người dùng 71: 11 lượt đánh giá, giá trị 5.0\n",
            "Người dùng 14: 5 lượt đánh giá, giá trị 5.0\n"
          ]
        },
        {
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 45.0 failed 1 times, most recent failure: Lost task 0.0 in stage 45.0 (TID 28) (KakaSheesh executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 17 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 17 more\r\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 497\u001b[0m\n\u001b[0;32m    494\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m final_model\n\u001b[0;32m    496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 497\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    498\u001b[0m     \u001b[38;5;66;03m# Dừng Spark Session khi hoàn thành\u001b[39;00m\n\u001b[0;32m    499\u001b[0m     spark\u001b[38;5;241m.\u001b[39mstop()\n",
            "Cell \u001b[1;32mIn[3], line 373\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCác trường của dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    372\u001b[0m \u001b[38;5;66;03m# Phân tích users có constant ratings\u001b[39;00m\n\u001b[1;32m--> 373\u001b[0m constant_users \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_constant_rating_users\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;66;03m# Chia dataset thành training và test sets với tỷ lệ 8:2\u001b[39;00m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;66;03m# PySpark không có hàm train_test_split như scikit-learn\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;66;03m# Chúng ta sẽ sử dụng randomSplit\u001b[39;00m\n\u001b[0;32m    378\u001b[0m train_data, test_data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mrandomSplit([\u001b[38;5;241m0.8\u001b[39m, \u001b[38;5;241m0.2\u001b[39m], seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
            "Cell \u001b[1;32mIn[3], line 355\u001b[0m, in \u001b[0;36manalyze_constant_rating_users\u001b[1;34m(spark_df)\u001b[0m\n\u001b[0;32m    352\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNgười dùng \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow\u001b[38;5;241m.\u001b[39muser\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow\u001b[38;5;241m.\u001b[39mrating_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m lượt đánh giá, giá trị \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow\u001b[38;5;241m.\u001b[39mrating\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    354\u001b[0m \u001b[38;5;66;03m# Trả về danh sách user IDs\u001b[39;00m\n\u001b[1;32m--> 355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconstant_users_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatMap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\rdd.py:1833\u001b[0m, in \u001b[0;36mRDD.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[0;32m   1832\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1833\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
            "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
            "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
            "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 45.0 failed 1 times, most recent failure: Lost task 0.0 in stage 45.0 (TID 28) (KakaSheesh executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 17 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 17 more\r\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, avg, count, stddev, lit, abs as abs_spark, sum as sum_spark, when, isnan, collect_list\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, DoubleType\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.recommendation import ALS\n",
        "import numpy as np\n",
        "from scipy.stats import pearsonr\n",
        "import matplotlib.pyplot as plt\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "\n",
        "# Khởi tạo Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"CollaborativeFiltering\") \\\n",
        "    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "    .config(\"spark.executor.memory\", \"4g\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "class CollaborativeFiltering:\n",
        "    \"\"\"\n",
        "    Lớp triển khai thuật toán Collaborative Filtering sử dụng hệ số tương quan Pearson\n",
        "    để đo độ tương đồng giữa các người dùng và gợi ý sản phẩm với PySpark.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, N, spark_df):\n",
        "        \"\"\"\n",
        "        Hàm khởi tạo cho CollaborativeFiltering\n",
        "\n",
        "        Tham số:\n",
        "        - N: Số lượng người dùng tương tự cần xem xét\n",
        "        - spark_df: Spark DataFrame chứa dữ liệu ratings\n",
        "        \"\"\"\n",
        "        self.N = N\n",
        "        self.spark = spark_df.sparkSession\n",
        "        self.data = spark_df\n",
        "        self.user_item_matrix = None\n",
        "        self.user_similarity_matrix = None\n",
        "        self.valid_users = None\n",
        "        self.user_means = None\n",
        "\n",
        "        # Loại bỏ users không khách quan\n",
        "        self._remove_constant_rating_users()\n",
        "\n",
        "        # Xây dựng ma trận user-item\n",
        "        self._build_user_item_matrix()\n",
        "\n",
        "        # Calculate user means for normalization\n",
        "        self._calculate_user_means()\n",
        "\n",
        "        # Tính ma trận độ tương đồng\n",
        "        self._calculate_user_similarity()\n",
        "\n",
        "    def _remove_constant_rating_users(self):\n",
        "        \"\"\"\n",
        "        Loại bỏ các users có constant ratings (đánh giá tất cả items với cùng một rating value)\n",
        "        \"\"\"\n",
        "        # Tính độ lệch chuẩn của ratings cho mỗi user\n",
        "        user_ratings_std = self.data.groupBy(\"user\").agg(stddev(\"rating\").alias(\"std_rating\"))\n",
        "        \n",
        "        # Tìm users có std = 0 (constant ratings) hoặc NULL (chỉ có 1 rating)\n",
        "        constant_users = user_ratings_std.filter(\n",
        "            (col(\"std_rating\") == 0) | col(\"std_rating\").isNull()\n",
        "        ).select(\"user\").rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "        # Lấy tất cả user IDs từ DataFrame gốc\n",
        "        all_users = self.data.select(\"user\").distinct().rdd.flatMap(lambda x: x).collect()\n",
        "        \n",
        "        # Lưu lại danh sách users hợp lệ\n",
        "        self.valid_users = [u for u in all_users if u not in constant_users]\n",
        "\n",
        "        # Cập nhật data chỉ giữ lại valid users\n",
        "        self.data = self.data.filter(col(\"user\").isin(self.valid_users))\n",
        "\n",
        "    def _build_user_item_matrix(self):\n",
        "        \"\"\"\n",
        "        Xây dựng ma trận user-item từ dữ liệu\n",
        "        \"\"\"\n",
        "        # PySpark không có hàm pivot trực tiếp như pandas\n",
        "        # Chúng ta sẽ giữ dữ liệu trong dạng user-item-rating\n",
        "        # và sử dụng các phép join khi cần\n",
        "        self.user_item_matrix = self.data\n",
        "\n",
        "    def _calculate_user_means(self):\n",
        "        \"\"\"\n",
        "        Calculate mean rating for each user for normalization\n",
        "        \"\"\"\n",
        "        self.user_means = self.data.groupBy(\"user\").agg(avg(\"rating\").alias(\"mean_rating\"))\n",
        "\n",
        "    def _calculate_user_similarity(self):\n",
        "        \"\"\"\n",
        "        Tính ma trận độ tương đồng giữa người dùng sử dụng hệ số tương quan Pearson\n",
        "        với dữ liệu đã được chuẩn hóa\n",
        "        \"\"\"\n",
        "        # Lấy danh sách users\n",
        "        users = self.valid_users\n",
        "        n_users = len(users)\n",
        "\n",
        "        # Tạo cấu trúc để lưu ma trận similarity\n",
        "        similarity_rows = []\n",
        "\n",
        "        # Join dữ liệu với user means để chuẩn hóa ratings\n",
        "        normalized_ratings = self.data.join(\n",
        "            self.user_means,\n",
        "            on=\"user\",\n",
        "            how=\"inner\"\n",
        "        ).withColumn(\n",
        "            \"normalized_rating\", \n",
        "            col(\"rating\") - col(\"mean_rating\")\n",
        "        )\n",
        "\n",
        "        # Chuyển normalized_ratings thành RDD để tính toán hiệu quả hơn\n",
        "        # Mỗi phần tử có dạng ((user, item), normalized_rating)\n",
        "        normalized_ratings_rdd = normalized_ratings.select(\n",
        "            \"user\", \"item\", \"normalized_rating\"\n",
        "        ).rdd.map(\n",
        "            lambda row: ((row.user, row.item), row.normalized_rating)\n",
        "        ).cache()\n",
        "\n",
        "        # Tính pearson correlation\n",
        "        # Đây là phần tính toán chính, chúng ta sẽ sử dụng map-reduce pattern\n",
        "        # Vì cần tính tương quan giữa từng cặp user, và PySpark không có hàm pearsonr sẵn,\n",
        "        # chúng ta cần một cách tiếp cận khác\n",
        "\n",
        "        # Đầu tiên, thu thập ratings của mỗi user\n",
        "        user_ratings_dict = {}\n",
        "        for user in users:\n",
        "            user_data = normalized_ratings.filter(col(\"user\") == user).select(\n",
        "                \"item\", \"normalized_rating\"\n",
        "            ).rdd.collectAsMap()\n",
        "            user_ratings_dict[user] = user_data\n",
        "\n",
        "        # Sau đó, tính toán tương quan cho mỗi cặp user\n",
        "        for i, user_i in enumerate(users):\n",
        "            row = {user_j: 0.0 for user_j in users}\n",
        "            row[user_i] = 1.0  # User hoàn toàn tương đồng với chính họ\n",
        "            \n",
        "            for j, user_j in enumerate(users):\n",
        "                if i < j:  # Chỉ tính nửa trên của ma trận (ma trận đối xứng)\n",
        "                    ratings_i = user_ratings_dict[user_i]\n",
        "                    ratings_j = user_ratings_dict[user_j]\n",
        "                    \n",
        "                    # Tìm các sản phẩm được đánh giá bởi cả hai người dùng\n",
        "                    common_items = set(ratings_i.keys()) & set(ratings_j.keys())\n",
        "                    \n",
        "                    if len(common_items) > 1:  # Cần ít nhất 2 sản phẩm chung\n",
        "                        # Get ratings\n",
        "                        user_i_ratings = [ratings_i[item] for item in common_items]\n",
        "                        user_j_ratings = [ratings_j[item] for item in common_items]\n",
        "                        \n",
        "                        # Calculate standard deviations\n",
        "                        std_i = float(np.std(user_i_ratings))\n",
        "                        std_j = float(np.std(user_j_ratings))\n",
        "                        \n",
        "                        # Check if either array is constant after normalization\n",
        "                        if std_i > 1e-10 and std_j > 1e-10:  # Using a small epsilon instead of 0\n",
        "                            # Tính hệ số tương quan Pearson\n",
        "                            try:\n",
        "                                correlation, _ = pearsonr(user_i_ratings, user_j_ratings)\n",
        "                                if not np.isnan(correlation):\n",
        "                                    # Chuyển đổi numpy.float64 thành float Python thông thường\n",
        "                                    row[user_j] = float(correlation)\n",
        "                                    # Vì ma trận đối xứng\n",
        "                            except Exception:\n",
        "                                pass  # Xử lý lỗi, giữ nguyên giá trị 0\n",
        "            \n",
        "            similarity_rows.append(row)\n",
        "\n",
        "        # Chuyển đổi về dạng DataFrame\n",
        "        similarity_schema = StructType([\n",
        "            StructField(\"user_i\", IntegerType(), False)\n",
        "        ] + [\n",
        "            StructField(f\"user_{user}\", FloatType(), True) for user in users\n",
        "        ])\n",
        "        \n",
        "        # Tạo DataFrame similarity\n",
        "        similarity_data = [(user_i,) + tuple(row[user_j] for user_j in users) \n",
        "                          for user_i, row in zip(users, similarity_rows)]\n",
        "        \n",
        "        # Tạo DataFrame từ dữ liệu\n",
        "        # Sử dụng createDataFrame thay vì createDataset vì không cần type inference\n",
        "        self.user_similarity_matrix = spark.createDataFrame(similarity_data, \n",
        "                                                          schema=similarity_schema)\n",
        "\n",
        "    def predict(self, user_id, num_recommendations):\n",
        "        \"\"\"\n",
        "        Dự đoán và gợi ý các sản phẩm cho người dùng\n",
        "        sử dụng dữ liệu chuẩn hóa\n",
        "\n",
        "        Tham số:\n",
        "        - user_id: ID của người dùng\n",
        "        - num_recommendations: Số lượng sản phẩm cần gợi ý\n",
        "\n",
        "        Trả về:\n",
        "        - DataFrame với các sản phẩm được sắp xếp theo điểm dự đoán giảm dần\n",
        "        \"\"\"\n",
        "        # Kiểm tra xem user có trong danh sách valid users không\n",
        "        if user_id not in self.valid_users:\n",
        "            print(f\"User {user_id} has constant ratings or not in training data\")\n",
        "            return self.spark.createDataFrame([], schema=StructType([\n",
        "                StructField(\"user\", IntegerType(), True),\n",
        "                StructField(\"item\", IntegerType(), True),\n",
        "                StructField(\"predicted_rating\", FloatType(), True)\n",
        "            ]))\n",
        "\n",
        "        # Lấy mean của user\n",
        "        user_mean = self.user_means.filter(col(\"user\") == user_id).select(\"mean_rating\").collect()[0][\"mean_rating\"]\n",
        "        \n",
        "        # Lấy tất cả items mà user đã đánh giá\n",
        "        user_rated_items = self.data.filter(col(\"user\") == user_id).select(\"item\").rdd.flatMap(lambda x: x).collect()\n",
        "        \n",
        "        # Lấy các items chưa được đánh giá (hiệu của tất cả items với items đã đánh giá)\n",
        "        all_items = self.data.select(\"item\").distinct().rdd.flatMap(lambda x: x).collect()\n",
        "        unrated_items = [item for item in all_items if item not in user_rated_items]\n",
        "        \n",
        "        # Lấy N người dùng tương tự nhất\n",
        "        # Lấy hàng tương ứng với user hiện tại trong ma trận similarity\n",
        "        similarity_row = self.user_similarity_matrix.filter(col(\"user_i\") == user_id).first()\n",
        "        \n",
        "        if similarity_row is None:\n",
        "            return self.spark.createDataFrame([], schema=StructType([\n",
        "                StructField(\"user\", IntegerType(), True),\n",
        "                StructField(\"item\", IntegerType(), True),\n",
        "                StructField(\"predicted_rating\", FloatType(), True)\n",
        "            ]))\n",
        "        \n",
        "        # Chuyển đổi similarity_row thành Dictionary để dễ sắp xếp\n",
        "        similarity_dict = {user: similarity_row[f\"user_{user}\"] for user in self.valid_users if user != user_id}\n",
        "        \n",
        "        # Sắp xếp theo similarity và lấy N người dùng tương tự nhất\n",
        "        top_similar_users = sorted(similarity_dict.items(), key=lambda x: x[1], reverse=True)[:self.N]\n",
        "        top_similar_users = [user for user, _ in top_similar_users]\n",
        "        \n",
        "        # Dự đoán rating cho mỗi item chưa được đánh giá\n",
        "        predictions = []\n",
        "        \n",
        "        # Lấy ratings của top_similar_users cho các sản phẩm chưa đánh giá\n",
        "        similar_users_ratings = self.data.filter(\n",
        "            col(\"user\").isin(top_similar_users) & col(\"item\").isin(unrated_items)\n",
        "        ).join(\n",
        "            self.user_means, \n",
        "            on=\"user\",\n",
        "            how=\"inner\"\n",
        "        ).withColumn(\n",
        "            \"normalized_rating\", \n",
        "            col(\"rating\") - col(\"mean_rating\")\n",
        "        )\n",
        "        \n",
        "        # Lấy similarity của mỗi user trong top_similar_users\n",
        "        similar_users_similarities = {user: similarity_dict[user] for user in top_similar_users}\n",
        "        \n",
        "        # Tính weighted sum cho mỗi item\n",
        "        for item in unrated_items:\n",
        "            item_ratings = similar_users_ratings.filter(col(\"item\") == item).collect()\n",
        "            \n",
        "            if len(item_ratings) > 0:\n",
        "                weighted_sum = 0.0\n",
        "                similarity_sum = 0.0\n",
        "                \n",
        "                for row in item_ratings:\n",
        "                    similarity = similar_users_similarities[row.user]\n",
        "                    weighted_sum += similarity * row.normalized_rating\n",
        "                    similarity_sum += abs(similarity)\n",
        "                \n",
        "                if similarity_sum > 0:\n",
        "                    predicted_rating_norm = weighted_sum / similarity_sum\n",
        "                    predicted_rating = predicted_rating_norm + user_mean\n",
        "                    \n",
        "                    predictions.append((user_id, item, float(predicted_rating)))\n",
        "        \n",
        "        # Tạo DataFrame chứa predictions\n",
        "        if predictions:\n",
        "            predictions_df = self.spark.createDataFrame(\n",
        "                predictions,\n",
        "                [\"user\", \"item\", \"predicted_rating\"]\n",
        "            )\n",
        "            \n",
        "            # Sắp xếp theo predicted_rating giảm dần và lấy top num_recommendations\n",
        "            predictions_df = predictions_df.orderBy(col(\"predicted_rating\").desc()).limit(num_recommendations)\n",
        "            return predictions_df\n",
        "        else:\n",
        "            return self.spark.createDataFrame([], schema=StructType([\n",
        "                StructField(\"user\", IntegerType(), True),\n",
        "                StructField(\"item\", IntegerType(), True),\n",
        "                StructField(\"predicted_rating\", FloatType(), True)\n",
        "            ]))\n",
        "\n",
        "def calculate_rmse_manual(actual_ratings, predicted_ratings):\n",
        "    \"\"\"\n",
        "    Tính RMSE\n",
        "    \n",
        "    Tham số:\n",
        "    - actual_ratings: List hoặc numpy array chứa giá trị thực tế\n",
        "    - predicted_ratings: List hoặc numpy array chứa giá trị dự đoán\n",
        "    \n",
        "    Trả về:\n",
        "    - Giá trị RMSE\n",
        "    \"\"\"\n",
        "    # Chuyển đổi sang numpy array nếu cần thiết\n",
        "    actual_arr = np.array(actual_ratings, dtype=float)\n",
        "    predicted_arr = np.array(predicted_ratings, dtype=float)\n",
        "    \n",
        "    # Tính toán RMSE\n",
        "    errors = actual_arr - predicted_arr\n",
        "    squared_errors = errors ** 2\n",
        "    mse = np.mean(squared_errors)\n",
        "    rmse = np.sqrt(mse)\n",
        "    \n",
        "    # Chuyển đổi từ numpy.float64 sang float thông thường\n",
        "    return float(rmse)\n",
        "\n",
        "def analyze_constant_rating_users(spark_df):\n",
        "    \"\"\"\n",
        "    Phân tích và thống kê users có constant ratings\n",
        "    \n",
        "    Tham số:\n",
        "    - spark_df: Spark DataFrame chứa dữ liệu ratings\n",
        "    \"\"\"\n",
        "    # Tính độ lệch chuẩn của ratings cho mỗi user\n",
        "    user_ratings_std = spark_df.groupBy(\"user\").agg(\n",
        "        stddev(\"rating\").alias(\"std_rating\"),\n",
        "        count(\"rating\").alias(\"rating_count\")\n",
        "    )\n",
        "    \n",
        "    # Tìm users có std = 0 (constant ratings) hoặc NULL (chỉ có 1 rating)\n",
        "    constant_users_df = user_ratings_std.filter(\n",
        "        (col(\"std_rating\") == 0) | col(\"std_rating\").isNull()\n",
        "    )\n",
        "    \n",
        "    # Thống kê\n",
        "    total_users = spark_df.select(\"user\").distinct().count()\n",
        "    constant_users_count = constant_users_df.count()\n",
        "    percentage = (constant_users_count / total_users) * 100\n",
        "    \n",
        "    print(f\"Tổng số lượng người dùng: {total_users}\")\n",
        "    print(f\"Số người dùng đánh giá không khách quan: {constant_users_count}\")\n",
        "    print(f\"Tỉ lệ phần trăm: {percentage:.2f}%\")\n",
        "    \n",
        "    # Hiển thị ví dụ\n",
        "    print(\"\\n Các người dùng đánh giá không khách quan:\")\n",
        "    \n",
        "    # Join để lấy giá trị rating\n",
        "    constant_users_with_ratings = constant_users_df.join(\n",
        "        spark_df,\n",
        "        on=\"user\",\n",
        "        how=\"inner\"\n",
        "    ).select(\"user\", \"rating_count\", \"rating\").distinct()\n",
        "    \n",
        "    # Lấy 80 người dùng đầu tiên\n",
        "    constant_users_sample = constant_users_with_ratings.limit(80).collect()\n",
        "    \n",
        "    for row in constant_users_sample:\n",
        "        print(f\"Người dùng {row.user}: {row.rating_count} lượt đánh giá, giá trị {row.rating}\")\n",
        "    \n",
        "    # Trả về danh sách user IDs\n",
        "    return constant_users_df.select(\"user\").rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "def main():\n",
        "    # Định nghĩa schema cho DataFrame\n",
        "    ratings_schema = StructType([\n",
        "        StructField(\"index\", IntegerType(), True),\n",
        "        StructField(\"user\", IntegerType(), True),\n",
        "        StructField(\"item\", IntegerType(), True),\n",
        "        StructField(\"rating\", FloatType(), True)\n",
        "    ])\n",
        "    \n",
        "    # Load dataset\n",
        "    data = spark.read.csv(\"ratings2k.csv\", header=True, schema=ratings_schema)\n",
        "    \n",
        "    print(f\"Cấu trúc của dataset: {data.count()} hàng x {len(data.columns)} cột\")\n",
        "    print(f\"Các trường của dataset: {data.columns}\")\n",
        "    \n",
        "    # Phân tích users có constant ratings\n",
        "    constant_users = analyze_constant_rating_users(data)\n",
        "    \n",
        "    # Chia dataset thành training và test sets với tỷ lệ 8:2\n",
        "    # PySpark không có hàm train_test_split như scikit-learn\n",
        "    # Chúng ta sẽ sử dụng randomSplit\n",
        "    train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\n",
        "    \n",
        "    print(f\"Training set size: {train_data.count()}\")\n",
        "    print(f\"Test set size: {test_data.count()}\")\n",
        "    \n",
        "    # Đánh giá thuật toán với N trong khoảng [2, 16]\n",
        "    print(\"\\nĐánh giá thuật toán với N từ 2 đến 16...\")\n",
        "    N_values = list(range(2, 17))\n",
        "    rmse_values = []\n",
        "    \n",
        "    for N in N_values:\n",
        "        print(f\"\\nN = {N}\")\n",
        "        \n",
        "        # Khởi tạo model với training data\n",
        "        cf_model = CollaborativeFiltering(N, train_data)\n",
        "        \n",
        "        # Dự đoán cho test set\n",
        "        test_users = test_data.select(\"user\").distinct().rdd.flatMap(lambda x: x).collect()\n",
        "        all_predictions = []\n",
        "        \n",
        "        for user in test_users:\n",
        "            # Bỏ qua users có constant ratings\n",
        "            if user not in cf_model.valid_users:\n",
        "                continue\n",
        "            \n",
        "            # Lấy tất cả ratings của user trong test set\n",
        "            user_test_ratings = test_data.filter(col(\"user\") == user)\n",
        "            user_test_count = user_test_ratings.count()\n",
        "            \n",
        "            # Dự đoán cho các items trong test set\n",
        "            predictions = cf_model.predict(user, user_test_count)\n",
        "            \n",
        "            if predictions.count() > 0:\n",
        "                # Chỉ giữ lại các predictions cho items có trong test set\n",
        "                merged = predictions.join(user_test_ratings, on=[\"user\", \"item\"], how=\"inner\")\n",
        "                \n",
        "                if merged.count() > 0:\n",
        "                    all_predictions.append(merged)\n",
        "        \n",
        "        # Tính RMSE\n",
        "        if all_predictions:\n",
        "            # Union tất cả DataFrames trong all_predictions\n",
        "            all_predictions_df = all_predictions[0]\n",
        "            for df in all_predictions[1:]:\n",
        "                all_predictions_df = all_predictions_df.union(df)\n",
        "            \n",
        "            if all_predictions_df.count() > 0:\n",
        "                # Collect data về driver để tính RMSE\n",
        "                actuals = all_predictions_df.select(\"rating\").rdd.flatMap(lambda x: x).collect()\n",
        "                predicteds = all_predictions_df.select(\"predicted_rating\").rdd.flatMap(lambda x: x).collect()\n",
        "                \n",
        "                rmse = calculate_rmse_manual(actuals, predicteds)\n",
        "                rmse_values.append(rmse)\n",
        "                print(f\"RMSE: {rmse:.4f}\")\n",
        "            else:\n",
        "                rmse_values.append(None)\n",
        "                print(\"Không có giá trị dự đoán hợp lệ\")\n",
        "        else:\n",
        "            rmse_values.append(None)\n",
        "            print(\"Không dự đoán được\")\n",
        "    \n",
        "    # Vẽ bar chart cho RMSE values với mỗi N\n",
        "    valid_indices = [i for i, rmse in enumerate(rmse_values) if rmse is not None]\n",
        "    valid_N = [N_values[i] for i in valid_indices]\n",
        "    valid_rmse = [float(rmse_values[i]) for i in valid_indices]  # Chuyển đổi sang float thông thường\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    bars = plt.bar(valid_N, valid_rmse, color='skyblue', edgecolor='black')\n",
        "    plt.xlabel('N (Số lượng người dùng cần xét)')\n",
        "    plt.ylabel('RMSE')\n",
        "    plt.title('Biểu đồ so sánh RMSE với các giá trị N khác nhau')\n",
        "    plt.xticks(valid_N)\n",
        "    plt.grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    for bar, rmse in zip(bars, valid_rmse):\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
        "                f'{rmse:.3f}', ha='center', va='bottom')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    best_N_index = int(np.argmin(valid_rmse))\n",
        "    best_N = valid_N[best_N_index]\n",
        "    best_rmse = float(valid_rmse[best_N_index])\n",
        "    print(f\"\\nVới N = {best_N} có giá trị RMSE tốt nhất = {best_rmse:.4f}\")\n",
        "    \n",
        "    # Tạo model cuối cùng với toàn bộ dataset để deploy\n",
        "    final_model = CollaborativeFiltering(best_N, data)\n",
        "    \n",
        "    # Demo the prediction functionality\n",
        "    print(\"\\n--- DEMO PREDICTION FOR SPECIFIC USERS ---\")\n",
        "    \n",
        "    # Choose a few users to demonstrate (pick from valid users)\n",
        "    demo_users = final_model.valid_users[:3]  # Get first 3 valid users\n",
        "    \n",
        "    for user_id in demo_users:\n",
        "        print(f\"\\nRecommendations for User {user_id}:\")\n",
        "        \n",
        "        # Get items user has already rated\n",
        "        user_rated_items = data.filter(col(\"user\") == user_id)\n",
        "        user_rated_count = user_rated_items.count()\n",
        "        print(f\"User has already rated {user_rated_count} items\")\n",
        "        \n",
        "        if user_rated_count > 0:\n",
        "            print(\"Sample of user's ratings:\")\n",
        "            user_rated_items.select(\"user\", \"item\", \"rating\").show(3)\n",
        "        \n",
        "        # Get recommendations for this user\n",
        "        recommendations = final_model.predict(user_id, 5)\n",
        "        \n",
        "        if recommendations.count() > 0:\n",
        "            print(\"\\nTop 5 recommended items:\")\n",
        "            recommendations.select(\"item\", \"predicted_rating\").show()\n",
        "        else:\n",
        "            print(\"No recommendations available for this user.\")\n",
        "    \n",
        "    return final_model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model = main()\n",
        "    # Dừng Spark Session khi hoàn thành\n",
        "    spark.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIFBf5wJsxUt"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
