{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Guc3FOLDIczU",
        "outputId": "3bfafbbb-a988-45d7-923c-41214289b925"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cấu trúc của dataset: (2365, 4)\n",
            "Các trường của dataset: ['index', 'user', 'item', 'rating']\n",
            "Training set size: 1894\n",
            "Test set size: 471\n",
            "\n",
            "Đánh giá thuật toán với N từ 2 đến 16...\n",
            "\n",
            "N = 2\n"
          ]
        },
        {
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 23.0 failed 1 times, most recent failure: Lost task 0.0 in stage 23.0 (TID 15) (KakaSheesh executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 17 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 17 more\r\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 554\u001b[0m\n\u001b[0;32m    551\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 554\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;66;03m# Đóng Spark session khi hoàn thành\u001b[39;00m\n\u001b[0;32m    557\u001b[0m     spark\u001b[38;5;241m.\u001b[39mstop()\n",
            "Cell \u001b[1;32mIn[2], line 450\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mN = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mN\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    449\u001b[0m \u001b[38;5;66;03m# Khởi tạo model với training data\u001b[39;00m\n\u001b[1;32m--> 450\u001b[0m cf_model \u001b[38;5;241m=\u001b[39m \u001b[43mCollaborativeFilteringSpark\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;66;03m# Lấy danh sách người dùng trong test set\u001b[39;00m\n\u001b[0;32m    453\u001b[0m test_users \u001b[38;5;241m=\u001b[39m test_data\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdistinct()\u001b[38;5;241m.\u001b[39mrdd\u001b[38;5;241m.\u001b[39mflatMap(\u001b[38;5;28;01mlambda\u001b[39;00m x: x)\u001b[38;5;241m.\u001b[39mcollect()\n",
            "Cell \u001b[1;32mIn[2], line 46\u001b[0m, in \u001b[0;36mCollaborativeFilteringSpark.__init__\u001b[1;34m(self, N, data_frame)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalized_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Loại bỏ users không khách quan\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_remove_constant_rating_users\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Tính rating trung bình cho mỗi user\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calculate_user_avg_ratings()\n",
            "Cell \u001b[1;32mIn[2], line 73\u001b[0m, in \u001b[0;36mCollaborativeFilteringSpark._remove_constant_rating_users\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     65\u001b[0m user_ratings_std \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mgroupBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39magg(\n\u001b[0;32m     66\u001b[0m     stddev(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrating\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstd_rating\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     67\u001b[0m     count(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrating\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount_rating\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     68\u001b[0m )\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Lọc ra các users có std = 0 (constant ratings) hoặc chỉ có 1 rating\u001b[39;00m\n\u001b[0;32m     71\u001b[0m constant_users \u001b[38;5;241m=\u001b[39m \u001b[43muser_ratings_std\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstd_rating\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misNull\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m|\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstd_rating\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m---> 73\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatMap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# In thông tin về constant users\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTổng số lượng người dùng: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mdistinct()\u001b[38;5;241m.\u001b[39mcount()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\rdd.py:1833\u001b[0m, in \u001b[0;36mRDD.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[0;32m   1832\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1833\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
            "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
            "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
            "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 23.0 failed 1 times, most recent failure: Lost task 0.0 in stage 23.0 (TID 15) (KakaSheesh executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 17 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:698)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:663)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:639)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:585)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:543)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 17 more\r\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, count, sum, avg, stddev, collect_list, explode, array, lit, expr, udf, desc, struct, rand\n",
        "from pyspark.sql.types import DoubleType, ArrayType, BooleanType, IntegerType, StructType, StructField, StringType, FloatType\n",
        "import math\n",
        "\n",
        "# Khởi tạo SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"CollaborativeFiltering\") \\\n",
        "    .config(\"spark.driver.memory\", \"8g\") \\\n",
        "    .config(\"spark.executor.memory\", \"8g\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Tạo schema để đọc dữ liệu\n",
        "schema = StructType([\n",
        "    StructField(\"index\", IntegerType(), True),\n",
        "    StructField(\"user\", IntegerType(), True),\n",
        "    StructField(\"item\", IntegerType(), True),\n",
        "    StructField(\"rating\", DoubleType(), True)\n",
        "])\n",
        "\n",
        "class CollaborativeFilteringSpark:\n",
        "    \"\"\"\n",
        "    Lớp triển khai thuật toán Collaborative Filtering sử dụng hệ số tương quan Pearson\n",
        "    để đo độ tương đồng giữa các người dùng và gợi ý sản phẩm trên nền tảng PySpark.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, N, data_frame):\n",
        "        \"\"\"\n",
        "        Hàm khởi tạo cho CollaborativeFilteringSpark\n",
        "\n",
        "        Tham số:\n",
        "        - N: Số lượng người dùng tương tự cần xem xét\n",
        "        - data_frame: DataFrame (PySpark) chứa dữ liệu ratings\n",
        "        \"\"\"\n",
        "        self.N = N\n",
        "        self.data = data_frame\n",
        "        self.user_item_matrix = None\n",
        "        self.user_similarity_matrix = None\n",
        "        self.valid_users = None\n",
        "        self.user_avg_ratings = None\n",
        "        self.normalized_data = None\n",
        "\n",
        "        # Loại bỏ users không khách quan\n",
        "        self._remove_constant_rating_users()\n",
        "\n",
        "        # Tính rating trung bình cho mỗi user\n",
        "        self._calculate_user_avg_ratings()\n",
        "\n",
        "        # Chuẩn hóa dữ liệu ratings\n",
        "        self._normalize_ratings()\n",
        "\n",
        "        # Xây dựng ma trận user-item\n",
        "        self._build_user_item_matrix()\n",
        "\n",
        "        # Tính ma trận độ tương đồng\n",
        "        self._calculate_user_similarity()\n",
        "\n",
        "    def _remove_constant_rating_users(self):\n",
        "        \"\"\"\n",
        "        Loại bỏ các users có constant ratings (đánh giá tất cả items với cùng một rating value)\n",
        "        \"\"\"\n",
        "        # Tính độ lệch chuẩn của ratings cho mỗi user\n",
        "        user_ratings_std = self.data.groupBy(\"user\").agg(\n",
        "            stddev(\"rating\").alias(\"std_rating\"),\n",
        "            count(\"rating\").alias(\"count_rating\")\n",
        "        )\n",
        "        \n",
        "        # Lọc ra các users có std = 0 (constant ratings) hoặc chỉ có 1 rating\n",
        "        constant_users = user_ratings_std.filter(\n",
        "            (col(\"std_rating\").isNull()) | (col(\"std_rating\") == 0)\n",
        "        ).select(\"user\").rdd.flatMap(lambda x: x).collect()\n",
        "        \n",
        "        # In thông tin về constant users\n",
        "        print(f\"Tổng số lượng người dùng: {self.data.select('user').distinct().count()}\")\n",
        "        print(f\"Số người dùng đánh giá không khách quan: {len(constant_users)}\")\n",
        "        print(f\"Tỉ lệ phần trăm: {len(constant_users) / self.data.select('user').distinct().count() * 100:.2f}%\")\n",
        "        \n",
        "        # In thông tin chi tiết về các constant users\n",
        "        print(\"\\n Các người dùng đánh giá không khách quan:\")\n",
        "        constant_user_info = self.data.filter(col(\"user\").isin(constant_users)) \\\n",
        "                                  .groupBy(\"user\") \\\n",
        "                                  .agg(count(\"rating\").alias(\"count_ratings\"), \n",
        "                                       avg(\"rating\").alias(\"avg_rating\")) \\\n",
        "                                  .collect()\n",
        "        \n",
        "        for row in constant_user_info[:8]:  # Giới hạn hiển thị 8 người dùng\n",
        "            print(f\"Người dùng {row['user']}: {row['count_ratings']} lượt đánh giá, giá trị {row['avg_rating']}\")\n",
        "        \n",
        "        # Lưu lại danh sách users hợp lệ\n",
        "        all_users = self.data.select(\"user\").distinct().rdd.flatMap(lambda x: x).collect()\n",
        "        self.valid_users = list(set(all_users) - set(constant_users))\n",
        "        \n",
        "        # Cập nhật data chỉ giữ lại valid users\n",
        "        self.data = self.data.filter(col(\"user\").isin(self.valid_users))\n",
        "\n",
        "    def _calculate_user_avg_ratings(self):\n",
        "        \"\"\"\n",
        "        Tính rating trung bình cho mỗi user\n",
        "        \"\"\"\n",
        "        self.user_avg_ratings = self.data.groupBy(\"user\").agg(\n",
        "            avg(\"rating\").alias(\"avg_rating\")\n",
        "        )\n",
        "\n",
        "    def _normalize_ratings(self):\n",
        "        \"\"\"\n",
        "        Chuẩn hóa ratings để xử lý vấn đề người dùng đánh giá khó tính và người dùng đánh giá dễ tính.\n",
        "        Phương pháp: Trừ đi rating trung bình của từng người dùng.\n",
        "        \"\"\"\n",
        "        # Join với rating trung bình của user\n",
        "        joined_data = self.data.join(\n",
        "            self.user_avg_ratings,\n",
        "            on=\"user\",\n",
        "            how=\"inner\"\n",
        "        )\n",
        "        \n",
        "        # Tính normalized rating = rating - avg_rating\n",
        "        self.normalized_data = joined_data.withColumn(\n",
        "            \"normalized_rating\",\n",
        "            col(\"rating\") - col(\"avg_rating\")\n",
        "        ).select(\"user\", \"item\", \"rating\", \"avg_rating\", \"normalized_rating\")\n",
        "        \n",
        "        # In thông tin về dữ liệu chuẩn hóa\n",
        "        print(\"\\nThông tin về dữ liệu chuẩn hóa:\")\n",
        "        normalized_stats = self.normalized_data.agg(\n",
        "            avg(\"normalized_rating\").alias(\"avg_normalized\"),\n",
        "            stddev(\"normalized_rating\").alias(\"std_normalized\"),\n",
        "            min(\"normalized_rating\").alias(\"min_normalized\"),\n",
        "            max(\"normalized_rating\").alias(\"max_normalized\")\n",
        "        ).collect()[0]\n",
        "        \n",
        "        print(f\"Giá trị trung bình normalized: {normalized_stats['avg_normalized']}\")\n",
        "        print(f\"Độ lệch chuẩn normalized: {normalized_stats['std_normalized']}\")\n",
        "        print(f\"Giá trị nhỏ nhất normalized: {normalized_stats['min_normalized']}\")\n",
        "        print(f\"Giá trị lớn nhất normalized: {normalized_stats['max_normalized']}\")\n",
        "\n",
        "    def _build_user_item_matrix(self):\n",
        "        \"\"\"\n",
        "        Xây dựng ma trận user-item từ dữ liệu đã chuẩn hóa\n",
        "        \"\"\"\n",
        "        # Sử dụng dữ liệu đã chuẩn hóa để xây dựng ma trận user-item\n",
        "        self.user_item_matrix = self.normalized_data.select(\"user\", \"item\", \"normalized_rating\")\n",
        "\n",
        "    def _calculate_user_similarity(self):\n",
        "        \"\"\"\n",
        "        Tính ma trận độ tương đồng giữa người dùng sử dụng hệ số tương quan Pearson\n",
        "        dựa trên dữ liệu đã chuẩn hóa\n",
        "        \"\"\"\n",
        "        # Tạo các vế để join\n",
        "        ratings1 = self.user_item_matrix.alias(\"r1\")\n",
        "        ratings2 = self.user_item_matrix.alias(\"r2\")\n",
        "        \n",
        "        # Join dữ liệu để tạo các cặp người dùng có cùng item\n",
        "        user_pairs = ratings1.join(\n",
        "            ratings2,\n",
        "            ratings1[\"item\"] == ratings2[\"item\"]\n",
        "        ).filter(\n",
        "            ratings1[\"user\"] < ratings2[\"user\"]  # Chỉ lấy nửa trên ma trận (vì đối xứng)\n",
        "        )\n",
        "        \n",
        "        # Tạo UDF để tính tương quan Pearson\n",
        "        def pearson_correlation(sum_xx, sum_xy, sum_yy, sum_x, sum_y, n):\n",
        "            \"\"\"\n",
        "            Tính hệ số tương quan Pearson giữa hai users\n",
        "            \"\"\"\n",
        "            if n < 2:  # Cần ít nhất 2 item chung\n",
        "                return 0.0\n",
        "                \n",
        "            denominator = math.sqrt(sum_xx - (sum_x ** 2) / n) * math.sqrt(sum_yy - (sum_y ** 2) / n)\n",
        "            \n",
        "            if denominator == 0:\n",
        "                return 0.0\n",
        "                \n",
        "            numerator = sum_xy - (sum_x * sum_y) / n\n",
        "            return numerator / denominator\n",
        "            \n",
        "        pearson_udf = udf(pearson_correlation, DoubleType())\n",
        "        \n",
        "        # Nhóm theo các cặp user và tính tương quan Pearson trên dữ liệu đã chuẩn hóa\n",
        "        similarities = user_pairs.groupBy(\n",
        "            col(\"r1.user\").alias(\"user_i\"),\n",
        "            col(\"r2.user\").alias(\"user_j\")\n",
        "        ).agg(\n",
        "            count(\"r1.item\").alias(\"n\"),\n",
        "            sum(col(\"r1.normalized_rating\") * col(\"r2.normalized_rating\")).alias(\"sum_xy\"),\n",
        "            sum(col(\"r1.normalized_rating\") * col(\"r1.normalized_rating\")).alias(\"sum_xx\"),\n",
        "            sum(col(\"r2.normalized_rating\") * col(\"r2.normalized_rating\")).alias(\"sum_yy\"),\n",
        "            sum(col(\"r1.normalized_rating\")).alias(\"sum_x\"),\n",
        "            sum(col(\"r2.normalized_rating\")).alias(\"sum_y\")\n",
        "        ).withColumn(\n",
        "            \"pearson\",\n",
        "            pearson_udf(\n",
        "                col(\"sum_xx\"), col(\"sum_xy\"), col(\"sum_yy\"),\n",
        "                col(\"sum_x\"), col(\"sum_y\"), col(\"n\")\n",
        "            )\n",
        "        ).select(\"user_i\", \"user_j\", \"pearson\")\n",
        "        \n",
        "        # Tạo ma trận đối xứng (thêm các cặp (user_j, user_i))\n",
        "        reversed_similarities = similarities.select(\n",
        "            col(\"user_j\").alias(\"user_i\"),\n",
        "            col(\"user_i\").alias(\"user_j\"),\n",
        "            col(\"pearson\")\n",
        "        )\n",
        "        \n",
        "        # Thêm tự tương quan (người dùng hoàn toàn tương đồng với chính họ)\n",
        "        self_similarities = spark.createDataFrame(\n",
        "            [(user, user, 1.0) for user in self.valid_users],\n",
        "            [\"user_i\", \"user_j\", \"pearson\"]\n",
        "        )\n",
        "        \n",
        "        # Kết hợp tất cả thành ma trận tương đồng đầy đủ\n",
        "        self.user_similarity_matrix = similarities.union(reversed_similarities).union(self_similarities)\n",
        "\n",
        "    def predict(self, user_ratings, num_recommendations=10):\n",
        "        \"\"\"\n",
        "        Dự đoán và gợi ý các sản phẩm cho người dùng\n",
        "        \n",
        "        Tham số:\n",
        "        - user_ratings: Vector ratings của người dùng (DataFrame hoặc dict)\n",
        "        - num_recommendations: Số lượng sản phẩm cần gợi ý\n",
        "        \n",
        "        Trả về:\n",
        "        - DataFrame (PySpark) với các sản phẩm được sắp xếp theo điểm dự đoán giảm dần\n",
        "        \"\"\"\n",
        "        # Xác định user_id từ user_ratings\n",
        "        user_id = None\n",
        "        \n",
        "        if isinstance(user_ratings, dict):\n",
        "            # Nếu là dict, trích xuất user_id trực tiếp\n",
        "            user_id = user_ratings.get(\"user\")\n",
        "            \n",
        "            # Nếu dict chứa các đánh giá, chuyển đổi thành DataFrame\n",
        "            if \"ratings\" in user_ratings:\n",
        "                user_items = []\n",
        "                for item_id, rating in user_ratings[\"ratings\"].items():\n",
        "                    user_items.append((user_id, int(item_id), float(rating)))\n",
        "                \n",
        "                if user_items:\n",
        "                    user_ratings_df = spark.createDataFrame(\n",
        "                        user_items, \n",
        "                        [\"user\", \"item\", \"rating\"]\n",
        "                    )\n",
        "                    # Cập nhật ratings cho người dùng này\n",
        "                    self.update_user_ratings(user_ratings_df)\n",
        "        \n",
        "        elif hasattr(user_ratings, \"select\"):\n",
        "            # Nếu là Spark DataFrame\n",
        "            try:\n",
        "                user_id = user_ratings.select(\"user\").first()[0]\n",
        "                self.update_user_ratings(user_ratings)\n",
        "            except:\n",
        "                pass\n",
        "        \n",
        "        # Nếu không xác định được user_id, trả về DataFrame rỗng\n",
        "        if user_id is None or user_id not in self.valid_users:\n",
        "            print(f\"User không hợp lệ hoặc không có trong danh sách valid users\")\n",
        "            return spark.createDataFrame([], schema=StructType([\n",
        "                StructField(\"user\", IntegerType(), True),\n",
        "                StructField(\"item\", IntegerType(), True),\n",
        "                StructField(\"predicted_rating\", DoubleType(), True)\n",
        "            ]))\n",
        "        \n",
        "        # Lấy N người dùng tương tự nhất\n",
        "        top_similar_users = self.user_similarity_matrix \\\n",
        "            .filter(col(\"user_i\") == user_id) \\\n",
        "            .filter(col(\"user_j\") != user_id) \\\n",
        "            .orderBy(col(\"pearson\").desc()) \\\n",
        "            .limit(self.N)\n",
        "        \n",
        "        # Lấy danh sách item đã được đánh giá bởi người dùng\n",
        "        rated_items = self.normalized_data \\\n",
        "            .filter(col(\"user\") == user_id) \\\n",
        "            .select(\"item\") \\\n",
        "            .rdd.flatMap(lambda x: x).collect()\n",
        "        \n",
        "        # Lấy ratings của những người dùng tương tự cho các item chưa được đánh giá\n",
        "        similar_users_ratings = self.normalized_data \\\n",
        "            .filter(col(\"user\").isin([row[\"user_j\"] for row in top_similar_users.collect()])) \\\n",
        "            .filter(~col(\"item\").isin(rated_items))\n",
        "        \n",
        "        # Join với thông tin độ tương đồng\n",
        "        joined_ratings = similar_users_ratings.join(\n",
        "            top_similar_users,\n",
        "            (similar_users_ratings[\"user\"] == top_similar_users[\"user_j\"]),\n",
        "            \"inner\"\n",
        "        )\n",
        "        \n",
        "        # Tính weighted normalized ratings\n",
        "        weighted_ratings = joined_ratings.select(\n",
        "            col(\"item\"),\n",
        "            (col(\"normalized_rating\") * col(\"pearson\")).alias(\"weighted_normalized_rating\"),\n",
        "            col(\"pearson\").alias(\"similarity\"),\n",
        "            col(\"avg_rating\").alias(\"avg_rating\")\n",
        "        )\n",
        "        \n",
        "        # Nhóm theo item và tính normalized rating dự đoán\n",
        "        predictions_normalized = weighted_ratings.groupBy(\"item\") \\\n",
        "            .agg(\n",
        "                sum(\"weighted_normalized_rating\").alias(\"sum_weighted_ratings\"),\n",
        "                sum(\"similarity\").alias(\"sum_similarities\"),\n",
        "                avg(\"avg_rating\").alias(\"avg_item_avg_ratings\")\n",
        "            ) \\\n",
        "            .filter(col(\"sum_similarities\") > 0) \\\n",
        "            .withColumn(\n",
        "                \"normalized_predicted_rating\", \n",
        "                col(\"sum_weighted_ratings\") / col(\"sum_similarities\")\n",
        "            )\n",
        "        \n",
        "        # Lấy rating trung bình của user hiện tại\n",
        "        user_avg = self.user_avg_ratings.filter(col(\"user\") == user_id).select(\"avg_rating\").first()\n",
        "        user_avg_rating = user_avg[\"avg_rating\"] if user_avg else 0\n",
        "        \n",
        "        # Tính predicted rating bằng cách thêm rating trung bình của user\n",
        "        predictions = predictions_normalized \\\n",
        "            .withColumn(\n",
        "                \"predicted_rating\",\n",
        "                col(\"normalized_predicted_rating\") + lit(user_avg_rating)\n",
        "            ) \\\n",
        "            .withColumn(\"user\", lit(user_id)) \\\n",
        "            .select(\"user\", \"item\", \"predicted_rating\") \\\n",
        "            .orderBy(col(\"predicted_rating\").desc()) \\\n",
        "            .limit(num_recommendations)\n",
        "        \n",
        "        return predictions\n",
        "\n",
        "    def update_user_ratings(self, user_ratings_df):\n",
        "        \"\"\"\n",
        "        Cập nhật ratings của người dùng trong dataset\n",
        "        \n",
        "        Tham số:\n",
        "        - user_ratings_df: DataFrame chứa ratings mới\n",
        "        \"\"\"\n",
        "        # Lấy user_id từ DataFrame\n",
        "        user_id = user_ratings_df.select(\"user\").first()[0]\n",
        "        \n",
        "        # Xóa ratings hiện tại của user\n",
        "        self.data = self.data.filter(col(\"user\") != user_id)\n",
        "        \n",
        "        # Thêm ratings mới vào\n",
        "        self.data = self.data.union(user_ratings_df)\n",
        "        \n",
        "        # Cập nhật rating trung bình\n",
        "        self._calculate_user_avg_ratings()\n",
        "        \n",
        "        # Cập nhật dữ liệu chuẩn hóa\n",
        "        self._normalize_ratings()\n",
        "        \n",
        "        # Cập nhật ma trận user-item\n",
        "        self._build_user_item_matrix()\n",
        "        \n",
        "        # Cập nhật ma trận tương đồng\n",
        "        self._calculate_user_similarity()\n",
        "        \n",
        "        # Thêm user vào valid_users nếu chưa có\n",
        "        if user_id not in self.valid_users:\n",
        "            self.valid_users.append(user_id)\n",
        "\n",
        "    def handle_cold_start(self, user_id, num_recommendations=10):\n",
        "        \"\"\"\n",
        "        Xử lý trường hợp người dùng mới không có đủ dữ liệu đánh giá\n",
        "        \n",
        "        Tham số:\n",
        "        - user_id: ID của người dùng\n",
        "        - num_recommendations: Số lượng sản phẩm cần gợi ý\n",
        "        \n",
        "        Trả về:\n",
        "        - DataFrame (PySpark) với các sản phẩm phổ biến nhất\n",
        "        \"\"\"\n",
        "        # Lấy danh sách item đã được đánh giá bởi người dùng\n",
        "        rated_items = []\n",
        "        if user_id in self.valid_users:\n",
        "            rated_items = self.data \\\n",
        "                .filter(col(\"user\") == user_id) \\\n",
        "                .select(\"item\") \\\n",
        "                .rdd.flatMap(lambda x: x).collect()\n",
        "        \n",
        "        # Tìm các sản phẩm phổ biến nhất (được đánh giá cao nhất)\n",
        "        popular_items = self.data \\\n",
        "            .groupBy(\"item\") \\\n",
        "            .agg(\n",
        "                count(\"rating\").alias(\"count\"),\n",
        "                avg(\"rating\").alias(\"avg_rating\")\n",
        "            ) \\\n",
        "            .filter(~col(\"item\").isin(rated_items)) \\\n",
        "            .orderBy(col(\"avg_rating\").desc(), col(\"count\").desc()) \\\n",
        "            .limit(num_recommendations) \\\n",
        "            .withColumn(\"user\", lit(user_id)) \\\n",
        "            .withColumn(\"predicted_rating\", col(\"avg_rating\")) \\\n",
        "            .select(\"user\", \"item\", \"predicted_rating\")\n",
        "            \n",
        "        return popular_items\n",
        "\n",
        "def calculate_rmse(actual_df, predicted_df):\n",
        "    \"\"\"\n",
        "    Tính RMSE từ hai DataFrame\n",
        "    \n",
        "    Tham số:\n",
        "    - actual_df: DataFrame chứa ratings thực tế\n",
        "    - predicted_df: DataFrame chứa ratings dự đoán\n",
        "    \n",
        "    Trả về:\n",
        "    - RMSE\n",
        "    \"\"\"\n",
        "    # Join hai DataFrame\n",
        "    joined_df = actual_df.join(\n",
        "        predicted_df,\n",
        "        [\"user\", \"item\"]\n",
        "    )\n",
        "    \n",
        "    # Tính bình phương sai số\n",
        "    joined_df = joined_df.withColumn(\n",
        "        \"squared_error\",\n",
        "        (col(\"rating\") - col(\"predicted_rating\")) * (col(\"rating\") - col(\"predicted_rating\"))\n",
        "    )\n",
        "    \n",
        "    # Tính RMSE\n",
        "    mse = joined_df.agg(avg(\"squared_error\")).collect()[0][0]\n",
        "    rmse = math.sqrt(mse) if mse is not None else None\n",
        "    \n",
        "    return rmse\n",
        "\n",
        "def main():\n",
        "    # Load dataset và in thông tin\n",
        "    data = spark.read.csv(\"ratings2k.csv\", header=True, schema=schema)\n",
        "    \n",
        "    print(f\"Cấu trúc của dataset: ({data.count()}, {len(data.columns)})\")\n",
        "    print(f\"Các trường của dataset: {data.columns}\")\n",
        "    \n",
        "    # Chia dữ liệu thành training và test sets với tỷ lệ 8:2 sử dụng PySpark\n",
        "    # Thêm cột ngẫu nhiên để chia dữ liệu\n",
        "    data_with_rand = data.withColumn(\"rand\", rand())\n",
        "    \n",
        "    # Chia dữ liệu dựa trên cột ngẫu nhiên\n",
        "    train_data = data_with_rand.filter(col(\"rand\") < 0.8).drop(\"rand\")\n",
        "    test_data = data_with_rand.filter(col(\"rand\") >= 0.8).drop(\"rand\")\n",
        "    \n",
        "    print(f\"Training set size: {train_data.count()}\")\n",
        "    print(f\"Test set size: {test_data.count()}\")\n",
        "    \n",
        "    # Đánh giá thuật toán với N trong khoảng [2, 16]\n",
        "    print(\"\\nĐánh giá thuật toán với N từ 2 đến 16...\")\n",
        "    N_values = list(range(2, 17))\n",
        "    rmse_values = []\n",
        "    \n",
        "    for N in N_values:\n",
        "        print(f\"\\nN = {N}\")\n",
        "        \n",
        "        # Khởi tạo model với training data\n",
        "        cf_model = CollaborativeFilteringSpark(N, train_data)\n",
        "        \n",
        "        # Lấy danh sách người dùng trong test set\n",
        "        test_users = test_data.select(\"user\").distinct().rdd.flatMap(lambda x: x).collect()\n",
        "        \n",
        "        # Lưu trữ tất cả predictions cho việc tính RMSE\n",
        "        all_predictions_list = []\n",
        "        \n",
        "        for user in test_users:\n",
        "            # Bỏ qua users có constant ratings\n",
        "            if user not in cf_model.valid_users:\n",
        "                continue\n",
        "                \n",
        "            # Lấy tất cả ratings của user trong test set\n",
        "            user_test_data = test_data.filter(col(\"user\") == user)\n",
        "            \n",
        "            # Dự đoán cho các item trong test set\n",
        "            user_test_items = user_test_data.select(\"item\").rdd.flatMap(lambda x: x).collect()\n",
        "            if not user_test_items:\n",
        "                continue\n",
        "                \n",
        "            # Lấy các ratings hiện tại của user từ training set\n",
        "            user_train_data = train_data.filter(col(\"user\") == user)\n",
        "            \n",
        "            # Dự đoán cho user này\n",
        "            predictions = cf_model.predict(user_train_data, num_recommendations=len(user_test_items))\n",
        "            \n",
        "            # Lấy các predictions cho các item trong test set\n",
        "            valid_predictions = predictions.filter(col(\"item\").isin(user_test_items))\n",
        "            \n",
        "            if valid_predictions.count() > 0:\n",
        "                all_predictions_list.append(valid_predictions)\n",
        "        \n",
        "        # Tính RMSE\n",
        "        if all_predictions_list:\n",
        "            # Gộp tất cả predictions lại\n",
        "            all_predictions_df = all_predictions_list[0]\n",
        "            for i in range(1, len(all_predictions_list)):\n",
        "                all_predictions_df = all_predictions_df.union(all_predictions_list[i])\n",
        "            \n",
        "            if all_predictions_df.count() > 0:\n",
        "                # Join với test data để lấy ratings thực tế\n",
        "                joined_preds = all_predictions_df.join(\n",
        "                    test_data.select(\"user\", \"item\", \"rating\"),\n",
        "                    [\"user\", \"item\"],\n",
        "                    \"inner\"\n",
        "                )\n",
        "                \n",
        "                if joined_preds.count() > 0:\n",
        "                    # Tính RMSE\n",
        "                    rmse = calculate_rmse(\n",
        "                        joined_preds.select(\"user\", \"item\", \"rating\"),\n",
        "                        joined_preds.select(\"user\", \"item\", \"predicted_rating\")\n",
        "                    )\n",
        "                    \n",
        "                    rmse_values.append(rmse)\n",
        "                    print(f\"RMSE: {rmse:.4f}\")\n",
        "                else:\n",
        "                    rmse_values.append(None)\n",
        "                    print(\"Không có dự đoán trùng với test set\")\n",
        "            else:\n",
        "                rmse_values.append(None)\n",
        "                print(\"Không có giá trị dự đoán hợp lệ\")\n",
        "        else:\n",
        "            rmse_values.append(None)\n",
        "            print(\"Không dự đoán được\")\n",
        "    \n",
        "    # Vẽ biểu đồ cho RMSE values bằng matplotlib\n",
        "    import matplotlib.pyplot as plt\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    valid_indices = [i for i, rmse in enumerate(rmse_values) if rmse is not None]\n",
        "    valid_N = [N_values[i] for i in valid_indices]\n",
        "    valid_rmse = [rmse_values[i] for i in valid_indices]\n",
        "    \n",
        "    bars = plt.bar(valid_N, valid_rmse, color='skyblue', edgecolor='black')\n",
        "    plt.xlabel('N (Số lượng người dùng cần xét)')\n",
        "    plt.ylabel('RMSE')\n",
        "    plt.title('Biểu đồ so sánh RMSE với các giá trị N khác nhau')\n",
        "    plt.xticks(valid_N)\n",
        "    plt.grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    for bar, rmse in zip(bars, valid_rmse):\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
        "                f'{rmse:.4f}', ha='center', va='bottom')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Tìm N với RMSE tốt nhất\n",
        "    if valid_rmse:\n",
        "        best_index = valid_rmse.index(min(valid_rmse))\n",
        "        best_N = valid_N[best_index]\n",
        "        best_rmse = valid_rmse[best_index]\n",
        "        print(f\"\\nVới N = {best_N} có giá trị RMSE tốt nhất = {best_rmse:.4f}\")\n",
        "    \n",
        "        # Tạo model cuối cùng với toàn bộ dataset để deploy\n",
        "        final_model = CollaborativeFilteringSpark(best_N, data)\n",
        "        return final_model\n",
        "    else:\n",
        "        print(\"\\nKhông thể xác định N tốt nhất do không có RMSE hợp lệ\")\n",
        "        return None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model = main()\n",
        "    \n",
        "    # Đóng Spark session khi hoàn thành\n",
        "    spark.stop()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
